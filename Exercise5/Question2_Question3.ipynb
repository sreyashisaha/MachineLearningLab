{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Variable Selection via Forward and Backward Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression,ElasticNet,Lasso\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import RidgeCV,Ridge, RidgeClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV,train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import Lars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset regression.npy, the dataset consists of over 100 predictors. We generated the regression dataset such that only a few predictors are relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('regression.npy', 'rb') as f:\n",
    "    X = np.load(f)\n",
    "    y = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feaures :\n",
      " [[0.19151945 0.62210877 0.43772774 ... 0.81920207 0.05711564 0.66942174]\n",
      " [0.76711663 0.70811536 0.79686718 ... 0.10310444 0.80237418 0.94555324]\n",
      " [0.97903882 0.88123225 0.62768192 ... 0.82215979 0.62796507 0.11792306]\n",
      " ...\n",
      " [0.94973721 0.43577786 0.01413315 ... 0.22343209 0.98627748 0.52625881]\n",
      " [0.50836    0.39705168 0.6062449  ... 0.93781016 0.14590475 0.35087649]\n",
      " [0.76700907 0.92954794 0.70505701 ... 0.52896505 0.76347965 0.07979716]]\n",
      "\n",
      " Target Variable:\n",
      " [15.48730667 22.07740941 17.43403019 ... 24.33341646 19.47937698\n",
      " 21.73607375]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFeaures :\\n\",X)\n",
    "print(\"\\n Target Variable:\\n\",y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform the following experiments using the least angle regression algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of The Covariates are or Dataset without the target variable: \t (10000, 100)\n",
      "\n",
      "Shape of the target variable: \t (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nShape of The Covariates are or Dataset without the target variable: \\t\",X.shape)\n",
    "print(\"\\nShape of the target variable: \\t\",y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a ones column to the dataset for the bias term.\n",
    "Splitting the data into 80:20 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X_Train Shape :  (8000, 101) \n",
      "X_Valid Shape :  (2000, 101) \n",
      "Y_Train Shape :  (8000,) \n",
      "Y_valid Shape :  (2000,)\n"
     ]
    }
   ],
   "source": [
    "n, m = X.shape\n",
    "split = int(0.8 * n)\n",
    "p = np.random.permutation(n)\n",
    "ones = np.ones(shape=X.shape[0]).reshape(-1, 1)\n",
    "x = np.concatenate((ones,X), 1)\n",
    "# Normalizing\n",
    "from sklearn import preprocessing\n",
    "x_data = preprocessing.normalize([x])\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_data, y, test_size=0.2,shuffle=True)\n",
    "print(\" X_Train Shape : \",x_train.shape,\"\\nX_Valid Shape : \",x_valid.shape,\"\\nY_Train Shape : \",y_train.shape,\"\\nY_valid Shape : \",y_valid.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Get the indices of the top k coefficient of these three models and compare them with the features selected via the forward and back search method. Feel free to try different values of k"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the top k coefficients and their respective indices(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Top_k_coeff_indices(arr, k):\n",
    "    # Finding the indices of the coeffcients in the array after partitioning\n",
    "    col_indices = np.argpartition(arr, k)[-k:]\n",
    "    # Getting the top k values \n",
    "    Top_k_coeff_values = arr[col_indices]\n",
    "    # Getting the positions of the top k coefficients from the array\n",
    "    Top_k_coeff_indices_positions = np.argsort(arr[col_indices])[::-1]\n",
    "    # Sorting the array of top k coefficients in descending order\n",
    "    sorted_Top_k_coeff_indices_values = np.sort(Top_k_coeff_values)\n",
    "    sorted_top_k_coeff_values = sorted_Top_k_coeff_indices_values[::-1]\n",
    "    print(\"\\n The maximum Coefficient values: \\n\")\n",
    "    print(sorted_top_k_coeff_values)\n",
    "    print(\"\\n The Indices of the maximum Coefficient values: \\n\")\n",
    "    print(Top_k_coeff_indices_positions)\n",
    "    # return sorted_top_k_coeff_values, Top_k_coeff_indices_positions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Forward Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FORWARD SEARCH IMPLEMENTATION FROM SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_search(model, x_train, y_train, x_valid, y_valid):\n",
    "    N,M = x_train.shape\n",
    "    print(f\"Total Number of Features:{M}\")\n",
    "    covariates_list = set(range(M))\n",
    "    features = set()\n",
    "    initial_error = 1e10\n",
    "    best_error = 1e10\n",
    "    best_feature = 1\n",
    "    while best_feature is not None:\n",
    "        print(\"Best MSE:\", best_error, \"\\tFinal Features:\", features)\n",
    "        best_feature = None\n",
    "        best_error = initial_error\n",
    "        for f in covariates_list - features:\n",
    "            # Concatenate Covariates columns\n",
    "            add_new_feature = list(features | {f})\n",
    "            # Train model till the concatenated covariates\n",
    "            model.fit(x_train[:, add_new_feature], y_train)\n",
    "            # Predict y_hat for validation set till the concatenated covariates\n",
    "            y_hat= model.predict(x_valid[:, add_new_feature])\n",
    "            # Compute the error for verification if the covariate is significant for the model or not\n",
    "            error_computed = mse(y_valid, y_hat)\n",
    "            if error_computed < best_error:\n",
    "                best_feature = f\n",
    "                best_error = error_computed\n",
    "        if best_error < initial_error:\n",
    "            features = features | {best_feature}\n",
    "            print(\"Important features obtained: \",features)\n",
    "            initial_error = best_error\n",
    "    print(\" The top 5 coefficients along with the idices\")\n",
    "    return Top_k_coeff_indices(model.coef_,5)\n",
    "    # return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Features:101\n",
      "Best MSE: 10000000000.0 \tFinal Features: set()\n",
      "Important features obtained:  {4}\n",
      "Best MSE: 15.83745742162434 \tFinal Features: {4}\n",
      "Important features obtained:  {1, 4}\n",
      "Best MSE: 11.835015836818465 \tFinal Features: {1, 4}\n",
      "Important features obtained:  {1, 2, 4}\n",
      "Best MSE: 8.36812921598756 \tFinal Features: {1, 2, 4}\n",
      "Important features obtained:  {1, 2, 4, 5}\n",
      "Best MSE: 5.86230330917619 \tFinal Features: {1, 2, 4, 5}\n",
      "Important features obtained:  {1, 2, 4, 5, 56}\n",
      "Best MSE: 5.8555060911605175 \tFinal Features: {1, 2, 4, 5, 56}\n",
      "Important features obtained:  {1, 2, 4, 5, 36, 56}\n",
      "Best MSE: 5.850436438271824 \tFinal Features: {1, 2, 4, 5, 36, 56}\n",
      "Important features obtained:  {1, 2, 4, 5, 36, 56, 91}\n",
      "Best MSE: 5.845381730293763 \tFinal Features: {1, 2, 4, 5, 36, 56, 91}\n",
      "Important features obtained:  {1, 2, 4, 5, 36, 56, 91, 63}\n",
      "Best MSE: 5.840286899663979 \tFinal Features: {1, 2, 4, 5, 36, 56, 91, 63}\n",
      "Important features obtained:  {1, 2, 4, 5, 36, 56, 91, 31, 63}\n",
      "Best MSE: 5.8364538555889816 \tFinal Features: {1, 2, 4, 5, 36, 56, 91, 31, 63}\n",
      "Important features obtained:  {1, 2, 4, 5, 36, 56, 90, 91, 31, 63}\n",
      "Best MSE: 5.8334324167676845 \tFinal Features: {1, 2, 4, 5, 36, 56, 90, 91, 31, 63}\n",
      "Important features obtained:  {1, 2, 4, 5, 36, 23, 56, 90, 91, 31, 63}\n",
      "Best MSE: 5.830668636628019 \tFinal Features: {1, 2, 4, 5, 36, 23, 56, 90, 91, 31, 63}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 36, 23, 56, 90, 91, 31, 63}\n",
      "Best MSE: 5.828046781831748 \tFinal Features: {1, 2, 3, 4, 5, 36, 23, 56, 90, 91, 31, 63}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 36, 23, 56, 90, 91, 31, 61, 63}\n",
      "Best MSE: 5.825724233107668 \tFinal Features: {1, 2, 3, 4, 5, 36, 23, 56, 90, 91, 31, 61, 63}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 36, 23, 56, 25, 90, 91, 31, 61, 63}\n",
      "Best MSE: 5.823741308886476 \tFinal Features: {1, 2, 3, 4, 5, 36, 23, 56, 25, 90, 91, 31, 61, 63}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 36, 79, 23, 56, 25, 90, 91, 31, 61, 63}\n",
      "Best MSE: 5.821960775258807 \tFinal Features: {1, 2, 3, 4, 5, 36, 79, 23, 56, 25, 90, 91, 31, 61, 63}\n",
      "Important features obtained:  {96, 1, 2, 3, 4, 5, 36, 79, 23, 56, 25, 90, 91, 31, 61, 63}\n",
      "Best MSE: 5.820304921309542 \tFinal Features: {96, 1, 2, 3, 4, 5, 36, 79, 23, 56, 25, 90, 91, 31, 61, 63}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 79, 23, 25, 90, 91, 31, 96, 36, 43, 56, 61, 63}\n",
      "Best MSE: 5.818668898304419 \tFinal Features: {1, 2, 3, 4, 5, 79, 23, 25, 90, 91, 31, 96, 36, 43, 56, 61, 63}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 74, 79, 23, 25, 90, 91, 31, 96, 36, 43, 56, 61, 63}\n",
      "Best MSE: 5.81722557634777 \tFinal Features: {1, 2, 3, 4, 5, 74, 79, 23, 25, 90, 91, 31, 96, 36, 43, 56, 61, 63}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 74, 79, 23, 25, 90, 91, 31, 96, 36, 43, 46, 56, 61, 63}\n",
      "Best MSE: 5.815809192723787 \tFinal Features: {1, 2, 3, 4, 5, 74, 79, 23, 25, 90, 91, 31, 96, 36, 43, 46, 56, 61, 63}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 74, 79, 23, 25, 90, 91, 31, 96, 36, 37, 43, 46, 56, 61, 63}\n",
      "Best MSE: 5.814585161038292 \tFinal Features: {1, 2, 3, 4, 5, 74, 79, 23, 25, 90, 91, 31, 96, 36, 37, 43, 46, 56, 61, 63}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 74, 79, 23, 25, 90, 91, 31, 96, 36, 37, 43, 46, 47, 56, 61, 63}\n",
      "Best MSE: 5.813419425458042 \tFinal Features: {1, 2, 3, 4, 5, 74, 79, 23, 25, 90, 91, 31, 96, 36, 37, 43, 46, 47, 56, 61, 63}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 74, 79, 23, 25, 90, 91, 31, 96, 36, 37, 43, 45, 46, 47, 56, 61, 63}\n",
      "Best MSE: 5.812305827502391 \tFinal Features: {1, 2, 3, 4, 5, 74, 79, 23, 25, 90, 91, 31, 96, 36, 37, 43, 45, 46, 47, 56, 61, 63}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 74, 79, 23, 25, 90, 91, 89, 31, 96, 36, 37, 43, 45, 46, 47, 56, 61, 63}\n",
      "Best MSE: 5.81139345727062 \tFinal Features: {1, 2, 3, 4, 5, 74, 79, 23, 25, 90, 91, 89, 31, 96, 36, 37, 43, 45, 46, 47, 56, 61, 63}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 74, 79, 23, 25, 90, 91, 89, 31, 96, 36, 37, 43, 45, 46, 47, 49, 56, 61, 63}\n",
      "Best MSE: 5.81044896757412 \tFinal Features: {1, 2, 3, 4, 5, 74, 79, 23, 25, 90, 91, 89, 31, 96, 36, 37, 43, 45, 46, 47, 49, 56, 61, 63}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 74, 79, 23, 25, 90, 91, 89, 31, 96, 36, 37, 43, 45, 46, 47, 49, 56, 59, 61, 63}\n",
      "Best MSE: 5.80957084714673 \tFinal Features: {1, 2, 3, 4, 5, 74, 79, 23, 25, 90, 91, 89, 31, 96, 36, 37, 43, 45, 46, 47, 49, 56, 59, 61, 63}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 72, 74, 79, 23, 25, 90, 91, 89, 31, 96, 36, 37, 43, 45, 46, 47, 49, 56, 59, 61, 63}\n",
      "Best MSE: 5.808711950386649 \tFinal Features: {1, 2, 3, 4, 5, 72, 74, 79, 23, 25, 90, 91, 89, 31, 96, 36, 37, 43, 45, 46, 47, 49, 56, 59, 61, 63}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 69, 72, 74, 79, 23, 25, 90, 91, 89, 31, 96, 36, 37, 43, 45, 46, 47, 49, 56, 59, 61, 63}\n",
      "Best MSE: 5.807897658806706 \tFinal Features: {1, 2, 3, 4, 5, 69, 72, 74, 79, 23, 25, 90, 91, 89, 31, 96, 36, 37, 43, 45, 46, 47, 49, 56, 59, 61, 63}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 69, 72, 74, 11, 79, 23, 25, 90, 91, 89, 31, 96, 36, 37, 43, 45, 46, 47, 49, 56, 59, 61, 63}\n",
      "Best MSE: 5.807163289406071 \tFinal Features: {1, 2, 3, 4, 5, 69, 72, 74, 11, 79, 23, 25, 90, 91, 89, 31, 96, 36, 37, 43, 45, 46, 47, 49, 56, 59, 61, 63}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 69, 72, 74, 11, 79, 83, 23, 25, 90, 91, 89, 31, 96, 36, 37, 43, 45, 46, 47, 49, 56, 59, 61, 63}\n",
      "Best MSE: 5.806484406498246 \tFinal Features: {1, 2, 3, 4, 5, 69, 72, 74, 11, 79, 83, 23, 25, 90, 91, 89, 31, 96, 36, 37, 43, 45, 46, 47, 49, 56, 59, 61, 63}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 69, 72, 74, 11, 79, 83, 23, 25, 90, 91, 89, 31, 96, 36, 37, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63}\n",
      "Best MSE: 5.806010572935058 \tFinal Features: {1, 2, 3, 4, 5, 69, 72, 74, 11, 79, 83, 23, 25, 90, 91, 89, 31, 96, 36, 37, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 69, 72, 74, 11, 77, 79, 83, 23, 25, 90, 91, 89, 31, 96, 36, 37, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63}\n",
      "Best MSE: 5.805545306612638 \tFinal Features: {1, 2, 3, 4, 5, 69, 72, 74, 11, 77, 79, 83, 23, 25, 90, 91, 89, 31, 96, 36, 37, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 69, 72, 74, 11, 77, 79, 83, 23, 25, 90, 91, 89, 31, 96, 35, 36, 37, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63}\n",
      "Best MSE: 5.805146540441176 \tFinal Features: {1, 2, 3, 4, 5, 69, 72, 74, 11, 77, 79, 83, 23, 25, 90, 91, 89, 31, 96, 35, 36, 37, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 11, 23, 25, 31, 35, 36, 37, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63, 69, 72, 74, 76, 77, 79, 83, 89, 90, 91, 96}\n",
      "Best MSE: 5.804762077226194 \tFinal Features: {1, 2, 3, 4, 5, 11, 23, 25, 31, 35, 36, 37, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63, 69, 72, 74, 76, 77, 79, 83, 89, 90, 91, 96}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 11, 23, 25, 31, 35, 36, 37, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 89, 90, 91, 96}\n",
      "Best MSE: 5.804417749998769 \tFinal Features: {1, 2, 3, 4, 5, 11, 23, 25, 31, 35, 36, 37, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 89, 90, 91, 96}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 11, 23, 25, 29, 31, 35, 36, 37, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 89, 90, 91, 96}\n",
      "Best MSE: 5.8041490907105375 \tFinal Features: {1, 2, 3, 4, 5, 11, 23, 25, 29, 31, 35, 36, 37, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 89, 90, 91, 96}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 11, 22, 23, 25, 29, 31, 35, 36, 37, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 89, 90, 91, 96}\n",
      "Best MSE: 5.80394694722308 \tFinal Features: {1, 2, 3, 4, 5, 11, 22, 23, 25, 29, 31, 35, 36, 37, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 89, 90, 91, 96}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 11, 21, 22, 23, 25, 29, 31, 35, 36, 37, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 89, 90, 91, 96}\n",
      "Best MSE: 5.803778449090137 \tFinal Features: {1, 2, 3, 4, 5, 11, 21, 22, 23, 25, 29, 31, 35, 36, 37, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 89, 90, 91, 96}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 11, 21, 22, 23, 25, 29, 31, 35, 36, 37, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 87, 89, 90, 91, 96}\n",
      "Best MSE: 5.803617814823707 \tFinal Features: {1, 2, 3, 4, 5, 11, 21, 22, 23, 25, 29, 31, 35, 36, 37, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 87, 89, 90, 91, 96}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 11, 21, 22, 23, 25, 29, 31, 35, 36, 37, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 87, 89, 90, 91, 94, 96}\n",
      "Best MSE: 5.803478199440784 \tFinal Features: {1, 2, 3, 4, 5, 11, 21, 22, 23, 25, 29, 31, 35, 36, 37, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 87, 89, 90, 91, 94, 96}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 11, 21, 22, 23, 25, 29, 31, 35, 36, 37, 42, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 87, 89, 90, 91, 94, 96}\n",
      "Best MSE: 5.803359341781577 \tFinal Features: {1, 2, 3, 4, 5, 11, 21, 22, 23, 25, 29, 31, 35, 36, 37, 42, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 87, 89, 90, 91, 94, 96}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 11, 21, 22, 23, 25, 29, 31, 35, 36, 37, 42, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 87, 89, 90, 91, 94, 96, 98}\n",
      "Best MSE: 5.803189261865784 \tFinal Features: {1, 2, 3, 4, 5, 11, 21, 22, 23, 25, 29, 31, 35, 36, 37, 42, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 87, 89, 90, 91, 94, 96, 98}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 11, 21, 22, 23, 25, 29, 31, 33, 35, 36, 37, 42, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 87, 89, 90, 91, 94, 96, 98}\n",
      "Best MSE: 5.80308502523006 \tFinal Features: {1, 2, 3, 4, 5, 11, 21, 22, 23, 25, 29, 31, 33, 35, 36, 37, 42, 43, 45, 46, 47, 48, 49, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 87, 89, 90, 91, 94, 96, 98}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 11, 21, 22, 23, 25, 29, 31, 33, 35, 36, 37, 42, 43, 45, 46, 47, 48, 49, 52, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 87, 89, 90, 91, 94, 96, 98}\n",
      "Best MSE: 5.802954279412803 \tFinal Features: {1, 2, 3, 4, 5, 11, 21, 22, 23, 25, 29, 31, 33, 35, 36, 37, 42, 43, 45, 46, 47, 48, 49, 52, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 87, 89, 90, 91, 94, 96, 98}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 10, 11, 21, 22, 23, 25, 29, 31, 33, 35, 36, 37, 42, 43, 45, 46, 47, 48, 49, 52, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 87, 89, 90, 91, 94, 96, 98}\n",
      "Best MSE: 5.802876886656875 \tFinal Features: {1, 2, 3, 4, 5, 10, 11, 21, 22, 23, 25, 29, 31, 33, 35, 36, 37, 42, 43, 45, 46, 47, 48, 49, 52, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 87, 89, 90, 91, 94, 96, 98}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 10, 11, 21, 22, 23, 25, 29, 31, 33, 35, 36, 37, 39, 42, 43, 45, 46, 47, 48, 49, 52, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 87, 89, 90, 91, 94, 96, 98}\n",
      "Best MSE: 5.8028249378795325 \tFinal Features: {1, 2, 3, 4, 5, 10, 11, 21, 22, 23, 25, 29, 31, 33, 35, 36, 37, 39, 42, 43, 45, 46, 47, 48, 49, 52, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 87, 89, 90, 91, 94, 96, 98}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 10, 11, 21, 22, 23, 25, 29, 31, 33, 35, 36, 37, 39, 42, 43, 45, 46, 47, 48, 49, 52, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 86, 87, 89, 90, 91, 94, 96, 98}\n",
      "Best MSE: 5.802794325968997 \tFinal Features: {1, 2, 3, 4, 5, 10, 11, 21, 22, 23, 25, 29, 31, 33, 35, 36, 37, 39, 42, 43, 45, 46, 47, 48, 49, 52, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 86, 87, 89, 90, 91, 94, 96, 98}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 10, 11, 21, 22, 23, 25, 29, 31, 33, 35, 36, 37, 39, 42, 43, 44, 45, 46, 47, 48, 49, 52, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 86, 87, 89, 90, 91, 94, 96, 98}\n",
      "Best MSE: 5.802781786949028 \tFinal Features: {1, 2, 3, 4, 5, 10, 11, 21, 22, 23, 25, 29, 31, 33, 35, 36, 37, 39, 42, 43, 44, 45, 46, 47, 48, 49, 52, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 86, 87, 89, 90, 91, 94, 96, 98}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 10, 11, 21, 22, 23, 25, 29, 31, 33, 35, 36, 37, 39, 42, 43, 44, 45, 46, 47, 48, 49, 52, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 86, 87, 89, 90, 91, 92, 94, 96, 98}\n",
      "Best MSE: 5.802767855657874 \tFinal Features: {1, 2, 3, 4, 5, 10, 11, 21, 22, 23, 25, 29, 31, 33, 35, 36, 37, 39, 42, 43, 44, 45, 46, 47, 48, 49, 52, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 86, 87, 89, 90, 91, 92, 94, 96, 98}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 10, 11, 19, 21, 22, 23, 25, 29, 31, 33, 35, 36, 37, 39, 42, 43, 44, 45, 46, 47, 48, 49, 52, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 86, 87, 89, 90, 91, 92, 94, 96, 98}\n",
      "Best MSE: 5.802759330105719 \tFinal Features: {1, 2, 3, 4, 5, 10, 11, 19, 21, 22, 23, 25, 29, 31, 33, 35, 36, 37, 39, 42, 43, 44, 45, 46, 47, 48, 49, 52, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 86, 87, 89, 90, 91, 92, 94, 96, 98}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 10, 11, 19, 21, 22, 23, 25, 29, 31, 33, 35, 36, 37, 39, 42, 43, 44, 45, 46, 47, 48, 49, 52, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 86, 87, 89, 90, 91, 92, 94, 96, 98, 100}\n",
      "Best MSE: 5.80275338711161 \tFinal Features: {1, 2, 3, 4, 5, 10, 11, 19, 21, 22, 23, 25, 29, 31, 33, 35, 36, 37, 39, 42, 43, 44, 45, 46, 47, 48, 49, 52, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 86, 87, 89, 90, 91, 92, 94, 96, 98, 100}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 6, 10, 11, 19, 21, 22, 23, 25, 29, 31, 33, 35, 36, 37, 39, 42, 43, 44, 45, 46, 47, 48, 49, 52, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 86, 87, 89, 90, 91, 92, 94, 96, 98, 100}\n",
      "Best MSE: 5.802748573924486 \tFinal Features: {1, 2, 3, 4, 5, 6, 10, 11, 19, 21, 22, 23, 25, 29, 31, 33, 35, 36, 37, 39, 42, 43, 44, 45, 46, 47, 48, 49, 52, 56, 59, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 86, 87, 89, 90, 91, 92, 94, 96, 98, 100}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 6, 10, 11, 19, 21, 22, 23, 25, 29, 31, 33, 35, 36, 37, 39, 42, 43, 44, 45, 46, 47, 48, 49, 52, 56, 59, 60, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 86, 87, 89, 90, 91, 92, 94, 96, 98, 100}\n",
      "Best MSE: 5.802747975013377 \tFinal Features: {1, 2, 3, 4, 5, 6, 10, 11, 19, 21, 22, 23, 25, 29, 31, 33, 35, 36, 37, 39, 42, 43, 44, 45, 46, 47, 48, 49, 52, 56, 59, 60, 61, 63, 66, 69, 72, 74, 76, 77, 79, 83, 86, 87, 89, 90, 91, 92, 94, 96, 98, 100}\n",
      " The top 5 coefficients along with the idices\n",
      "\n",
      " The maximum Coefficient values: \n",
      "\n",
      "[10.07086953  6.65138633  5.06005849  0.12167152  0.02895172]\n",
      "\n",
      " The Indices of the maximum Coefficient values: \n",
      "\n",
      "[2 4 1 3 0]\n"
     ]
    }
   ],
   "source": [
    "model = linear_model.Lars()\n",
    "forward_search( model ,x_train , y_train , x_valid , y_valid )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FORWARD SEARCH IMPLEMENTATION WITH SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Features obtained from Forward Search :\n",
      " [0, 1, 2, 3, 4, 6, 7, 12, 14, 17, 21, 22, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 40, 41, 42, 46, 47, 49, 50, 51, 54, 55, 60, 61, 62, 65, 69, 72, 74, 76, 78, 79, 80, 81, 84, 86, 90, 96, 97]\n",
      "\n",
      "Mean Squarred Error On The Features Obtained from Forward Search:\t  8.223846520097563\n"
     ]
    }
   ],
   "source": [
    "feature_selection_model = SequentialFeatureSelector(model, n_features_to_select= 50, direction=\"forward\")\n",
    "feature_selection_model.fit(X, y)\n",
    "features_boolean = feature_selection_model.get_support().tolist()\n",
    "feature_list = []\n",
    "for i in range(len(features_boolean)):\n",
    "    if features_boolean[i]:\n",
    "        feature_list.append(i)\n",
    "\n",
    "print(\"\\nBest Features obtained from Forward Search :\\n\", feature_list)\n",
    "# Training our data with the new set of features obtained from Forward Search\n",
    "lars_new_model = Lars()\n",
    "lars_new_model.fit(x_train[:,feature_list],y_train)\n",
    "y_hat = lars_new_model.predict(x_valid[:,feature_list])\n",
    "MSE_score = mean_squared_error(y_valid,y_hat)\n",
    "print(\"\\nMean Squarred Error On The Features Obtained from Forward Search:\\t \",MSE_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The maximum Coefficient values: \n",
      "\n",
      "[6.64179097 0.1956206  0.16666649 0.01025202 0.00666463]\n",
      "\n",
      " The Indices of the maximum Coefficient values: \n",
      "\n",
      "[3 2 1 0 4]\n"
     ]
    }
   ],
   "source": [
    "## TOP 5 Coefficients along with their indicies\n",
    "Top_k_coeff_indices(lars_new_model.coef_,5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backward Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BACKWARD SEARCH IMPLEMENTATION WITH SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Features obtained from Backward Search :\n",
      " [0, 1, 2, 3, 4, 6, 7, 12, 14, 17, 21, 22, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 40, 41, 42, 46, 47, 49, 50, 51, 54, 55, 60, 61, 62, 65, 69, 72, 74, 76, 78, 79, 80, 81, 84, 86, 90, 96, 97]\n",
      "\n",
      "Mean Squarred Error On The Features Obtained from Backward Search:\t  8.645598420338786\n"
     ]
    }
   ],
   "source": [
    "feature_selection_model = SequentialFeatureSelector(model, n_features_to_select= 50, direction=\"backward\")\n",
    "feature_selection_model.fit(X, y)\n",
    "features_boolean = feature_selection_model.get_support().tolist()\n",
    "feature_list = []\n",
    "for i in range(len(features_boolean)):\n",
    "    if features_boolean[i]:\n",
    "        feature_list.append(i)\n",
    "print(\"\\nBest Features obtained from Backward Search :\\n\", feature_list)\n",
    "# Training our data with the new set of features obtained from Backward Search\n",
    "lars_new_model = Lars()\n",
    "lars_new_model.fit(x_train[:,feature_list],y_train)\n",
    "y_hat = lars_new_model.predict(x_valid[:,feature_list])\n",
    "MSE_score = mean_squared_error(y_valid,y_hat)\n",
    "print(\"\\nMean Squarred Error On The Features Obtained from Backward Search:\\t \",MSE_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The maximum Coefficient values: \n",
      "\n",
      "[6.77221487 0.12951897 0.12025592 0.0783876  0.01658407]\n",
      "\n",
      " The Indices of the maximum Coefficient values: \n",
      "\n",
      "[3 1 2 4 0]\n"
     ]
    }
   ],
   "source": [
    "## TOP 5 Coefficients along with their indicies\n",
    "Top_k_coeff_indices(lars_new_model.coef_,5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BACKWARD SEARCH IMPLEMENTATION FROM SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_search(model, x_train, y_train, x_valid, y_valid):\n",
    "    N,M = x_train.shape\n",
    "    print(f\"Total Number of Features:{M}\")\n",
    "    covariates_list = set(range(M))\n",
    "    best_feature = 1\n",
    "    model.fit(x_train, y_train)\n",
    "    pred= model.predict(x_valid)\n",
    "    error = mse(y_valid, pred)\n",
    "    while best_feature is not None:\n",
    "        print(\"MSE(validation):\", error, \"    Features:\", covariates_list)\n",
    "        best_feature = None\n",
    "        best_error = error\n",
    "        for f in covariates_list:\n",
    "            add_new_feature = list(covariates_list-{f})\n",
    "            model.fit(x_train[:, :len(add_new_feature)], y_train)\n",
    "            pred = model.predict(x_valid[:, :len(add_new_feature)])\n",
    "            error_computed = mse(y_valid, pred)\n",
    "            if error_computed < best_error:\n",
    "                best_feature = f\n",
    "                best_error = error_computed\n",
    "        if best_error < error:\n",
    "            covariates_list = covariates_list - {best_feature}\n",
    "            print(\"Important features obtained: \",covariates_list)\n",
    "            error = best_error\n",
    "    print(\" The top 5 coefficients along with the idices\")\n",
    "    return Top_k_coeff_indices(model.coef_,5)\n",
    "    # return covariates_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Features:101\n",
      "MSE(validation): 5.92812398907075     Features: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100}\n",
      "Important features obtained:  {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100}\n",
      "MSE(validation): 5.928123074225292     Features: {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100}\n",
      " The top 5 coefficients along with the idices\n",
      "\n",
      " The maximum Coefficient values: \n",
      "\n",
      "[ 6.70648149  0.08836699  0.         -0.01224585 -0.01507864]\n",
      "\n",
      " The Indices of the maximum Coefficient values: \n",
      "\n",
      "[1 3 4 0 2]\n"
     ]
    }
   ],
   "source": [
    "model = linear_model.Lars()\n",
    "backward_search( model ,x_train , y_train , x_valid , y_valid )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable selection via forward and backward search drops some predictors; in some cases, we don’t want to remove these predictors. Rather we want their coefficients to be small as possible. We are going to test the effect of the regularization term alpha. \n",
    "### Try the following alpha values: [10, 1, 0.1, 0.0001, 0.00001 ], use regression.npy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = {'alpha':[10, 1, 0.1, 0.0001, 0.00001]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Grid Search\n",
    "### a)Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The Best Hyperparameter for Ridge Regression from the given list of alpha values is : \n",
      "\n",
      "{'alpha': 1}\n",
      "The Mean Squarred Error for Ridge Regression is :\t  5.928912324660127\n",
      "\n",
      " The maximum Coefficient values: \n",
      "\n",
      "[6.69635381e+00 1.06661918e-01 8.84709707e-02 1.89214737e-02\n",
      " 5.48353351e-03]\n",
      "\n",
      " The Indices of the maximum Coefficient values: \n",
      "\n",
      "[2 0 1 4 3]\n"
     ]
    }
   ],
   "source": [
    "# define the model/ estimator\n",
    "model = Ridge()\n",
    "\n",
    "# define the grid search\n",
    "Ridge_reg= GridSearchCV(model, alphas,scoring='neg_mean_squared_error',cv=5)\n",
    "#fit the grid search\n",
    "Ridge_reg.fit(x_train,y_train)\n",
    "# best estimator\n",
    "print(\"\\n The Best Hyperparameter for Ridge Regression from the given list of alpha values is : \\n\")\n",
    "print(Ridge_reg.best_params_)\n",
    "\n",
    "# Fitting the model with the best hyperparameter obtained\n",
    "model = Ridge(alpha = Ridge_reg.best_params_['alpha'])\n",
    "model.fit(x_train, y_train)\n",
    "yhat = model.predict(x_valid)\n",
    "mse = mean_squared_error(y_valid, yhat)\n",
    "print(\"The Mean Squarred Error for Ridge Regression is :\\t \",mse)\n",
    "\n",
    "Top_k_coeff_indices(model.coef_,5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The Best Hyperparameter for Lasso Regression from the given list of alpha values is : \n",
      "\n",
      "{'alpha': 0.0001}\n",
      "The Mean Squarred Error for Lasso Regression is :\t  5.927321185786832\n",
      "\n",
      " The maximum Coefficient values: \n",
      "\n",
      "[6.70514057e+00 1.05630210e-01 8.73115691e-02 1.75780490e-02\n",
      " 4.43979327e-03]\n",
      "\n",
      " The Indices of the maximum Coefficient values: \n",
      "\n",
      "[2 0 1 4 3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define the model/ estimator\n",
    "model = Lasso()\n",
    "\n",
    "# define the grid search\n",
    "lasso= GridSearchCV(model, alphas,cv=5)\n",
    "#fit the grid search\n",
    "lasso.fit(x_train,y_train)\n",
    "# best estimator\n",
    "print(\"\\n The Best Hyperparameter for Lasso Regression from the given list of alpha values is : \\n\")\n",
    "print(lasso.best_params_)\n",
    "model = Lasso(alpha = lasso.best_params_['alpha'])\n",
    "model.fit(x_train, y_train)\n",
    "yhat = model.predict(x_valid)\n",
    "mse1 = mean_squared_error(y_valid, yhat)\n",
    "print(\"The Mean Squarred Error for Lasso Regression is :\\t \",mse1)\n",
    "\n",
    "Top_k_coeff_indices(model.coef_,5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The Best Hyperparameter for ElasticNet Regression from the given list of alpha values is : \n",
      "\n",
      "{'alpha': 0.0001}\n",
      "The Mean Squarred Error for ElasticNet Regression is :\t  5.92802582780389\n",
      "\n",
      " The maximum Coefficient values: \n",
      "\n",
      "[6.70173765e+00 1.06150351e-01 8.78922987e-02 1.82395330e-02\n",
      " 4.98138045e-03]\n",
      "\n",
      " The Indices of the maximum Coefficient values: \n",
      "\n",
      "[2 0 1 4 3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define the model/ estimator\n",
    "model = ElasticNet()\n",
    "\n",
    "# define the grid search\n",
    "elasticnet= GridSearchCV(model, alphas,cv=5)\n",
    "#fit the grid search\n",
    "elasticnet.fit(x_train,y_train)\n",
    "# best estimator\n",
    "print(\"\\n The Best Hyperparameter for ElasticNet Regression from the given list of alpha values is : \\n\")\n",
    "print(elasticnet.best_params_)\n",
    "model = ElasticNet(alpha = elasticnet.best_params_['alpha'])\n",
    "model.fit(x_train, y_train)\n",
    "yhat = model.predict(x_valid)\n",
    "mse2 = mean_squared_error(y_valid, yhat)\n",
    "print(\"The Mean Squarred Error for ElasticNet Regression is :\\t \",mse2)\n",
    "\n",
    "Top_k_coeff_indices(model.coef_,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Random Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The Best Hyperparameter for Ridge Regression from the given list of alpha values is : \n",
      "\n",
      "{'alpha': 1}\n",
      "The Mean Squarred Error for Ridge Regression is :\t  5.928912324660127\n",
      "\n",
      " The maximum Coefficient values: \n",
      "\n",
      "[6.69635381e+00 1.06661918e-01 8.84709707e-02 1.89214737e-02\n",
      " 5.48353351e-03]\n",
      "\n",
      " The Indices of the maximum Coefficient values: \n",
      "\n",
      "[2 0 1 4 3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define the model/ estimator\n",
    "model = Ridge()\n",
    "\n",
    "# define the grid search\n",
    "Ridge_reg= RandomizedSearchCV(model, alphas,scoring='neg_mean_squared_error',cv=5)\n",
    "#fit the grid search\n",
    "Ridge_reg.fit(x_train,y_train)\n",
    "# best estimator\n",
    "print(\"\\n The Best Hyperparameter for Ridge Regression from the given list of alpha values is : \\n\")\n",
    "print(Ridge_reg.best_params_)\n",
    "\n",
    "model = Ridge(alpha = Ridge_reg.best_params_['alpha'])\n",
    "model.fit(x_train, y_train)\n",
    "yhat = model.predict(x_valid)\n",
    "mse3 = mean_squared_error(y_valid, yhat)\n",
    "print(\"The Mean Squarred Error for Ridge Regression is :\\t \",mse3)\n",
    "\n",
    "Top_k_coeff_indices(model.coef_,5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The Best Hyperparameter for Lasso Regression from the given list of alpha values is : \n",
      "\n",
      "{'alpha': 0.0001}\n",
      "The Mean Squarred Error for Ridge Regression is :\t  5.927321185786832\n",
      "\n",
      " The maximum Coefficient values: \n",
      "\n",
      "[6.70514057e+00 1.05630210e-01 8.73115691e-02 1.75780490e-02\n",
      " 4.43979327e-03]\n",
      "\n",
      " The Indices of the maximum Coefficient values: \n",
      "\n",
      "[2 0 1 4 3]\n"
     ]
    }
   ],
   "source": [
    "# define the model/ estimator\n",
    "model = Lasso()\n",
    "\n",
    "# define the grid search\n",
    "lasso= RandomizedSearchCV(model, alphas,cv=5)\n",
    "#fit the grid search\n",
    "lasso.fit(x_train,y_train)\n",
    "# best estimator\n",
    "print(\"\\n The Best Hyperparameter for Lasso Regression from the given list of alpha values is : \\n\")\n",
    "print(lasso.best_params_)\n",
    "model = Lasso(alpha = lasso.best_params_['alpha'])\n",
    "model.fit(x_train, y_train)\n",
    "yhat = model.predict(x_valid)\n",
    "mse4 = mean_squared_error(y_valid, yhat)\n",
    "print(\"The Mean Squarred Error for Ridge Regression is :\\t \",mse4)\n",
    "\n",
    "Top_k_coeff_indices(model.coef_,5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii) Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The Best Hyperparameter for Lasso Regression from the given list of alpha values is : \n",
      "\n",
      "{'alpha': 0.0001}\n",
      "The Mean Squarred Error for Ridge Regression is :\t  5.92802582780389\n",
      "\n",
      " The maximum Coefficient values: \n",
      "\n",
      "[6.70173765e+00 1.06150351e-01 8.78922987e-02 1.82395330e-02\n",
      " 4.98138045e-03]\n",
      "\n",
      " The Indices of the maximum Coefficient values: \n",
      "\n",
      "[2 0 1 4 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "# define the model/ estimator\n",
    "model = ElasticNet()\n",
    "\n",
    "# define the grid search\n",
    "elasticnet= RandomizedSearchCV(model, alphas,cv=5)\n",
    "#fit the grid search\n",
    "elasticnet.fit(x_train,y_train)\n",
    "# best estimator\n",
    "print(\"\\n The Best Hyperparameter for Lasso Regression from the given list of alpha values is : \\n\")\n",
    "print(elasticnet.best_params_)\n",
    "model = ElasticNet(alpha = elasticnet.best_params_['alpha'])\n",
    "model.fit(x_train, y_train)\n",
    "yhat = model.predict(x_valid)\n",
    "mse5 = mean_squared_error(y_valid, yhat)\n",
    "print(\"The Mean Squarred Error for Ridge Regression is :\\t \",mse5)\n",
    "\n",
    "Top_k_coeff_indices(model.coef_,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " All the mses for the 6 models are as follows:\n",
      "\n",
      "\n",
      "MSE1:  5.928912324660127 \n",
      "\n",
      "\n",
      "MSE2:  5.927321185786832 \n",
      "\n",
      "\n",
      "MSE3:  5.92802582780389 \n",
      "\n",
      "\n",
      "MSE4:  5.928912324660127 \n",
      "\n",
      "\n",
      "MSE5:  5.927321185786832 \n",
      "\n",
      "\n",
      "MSE6:  5.92802582780389 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n All the mses for the 6 models are as follows:\\n\")\n",
    "print(\"\\nMSE1: \",mse,\"\\n\")\n",
    "print(\"\\nMSE2: \",mse1,\"\\n\")\n",
    "print(\"\\nMSE3: \",mse2,\"\\n\")\n",
    "print(\"\\nMSE4: \",mse3,\"\\n\")\n",
    "print(\"\\nMSE5: \",mse4,\"\\n\")\n",
    "print(\"\\nMSE6: \",mse5,\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Briefly discuss the effect of high and low values of alpha. Then, return the best three models with their respective alpha values using the appropriate metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best 3 models are as follows:\n",
      "\n",
      "\n",
      "MSE1:  5.928912324660127 \n",
      "\n",
      "\n",
      "MSE2:  5.927321185786832 \n",
      "\n",
      "\n",
      "MSE3:  5.92802582780389 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Best 3 models are as follows:\\n\")\n",
    "print(\"\\nMSE1: \",mse,\"\\n\")\n",
    "print(\"\\nMSE2: \",mse1,\"\\n\")\n",
    "print(\"\\nMSE3: \",mse2,\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge or Lasso regression is basically Shrinkage(regularization) techniques, which uses different parameters and values to shrink or penalize the coefficients.When we fit a model, we are asking it to learn a set of coefficients that best fit over the training distribution as well as hope to generalize on test data points as well. Learning those coefficients can be done in various ways and multiple techniques are there to reduce the error in coefficients such as LMS(Least mean Squared), RSS(Residual Sum of Squares).\n",
    "\n",
    "For Ridge Regression ⍺ given is a parameter in the ridge function. So, by changing the values of alpha, we are controlling the penalty term. The higher the values of alpha, the bigger ⍺ is the penalty and therefore the magnitude of coefficients is reduced.\n",
    "\n",
    "For Lasso Regression the increasing value of the regularization parameter means increasing regularization strength, the absolute values of weights would need to decrease (shrink) to keep the overall value of the loss function minimized. The optimization of the Lasso loss function results in some of the weights becoming zero and hence can be seen as a method of selection of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compare the best three models to the models from question 2. What do you observe?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both Random Search and Grid Search gives us the same value for Mean Squarred Error in all the three cases that is Ridge, Lasso and ElasticNet, therefore, I have just taken the 3 models from Grid Search for comparison with the model in question 2.\n",
    "\n",
    "We observe that the MSE obtained after performing forward search is more compared to the MSEs obtained from Grid Search for Ridge, Lasso and Elastic net. The MSEs for grid search are printed above and the MSE obtained in forward search is 8.223846520097563 which is more compared to the others.\n",
    "\n",
    "We also observe that MSE obtained after performing the backward search is also more compared to the MSEs obtained from Grid Search for Ridge, Lasso and Elasticnet. The MSE for backward search is observed as 8.645598420338786."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Get the indices of the top k coefficient of these three models and compare them with the features selected via the forward and back search method. Feel free to try different values of k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Forward Search obtained from above:\n",
    "\n",
    "The maximum Coefficient values: \n",
    "\n",
    "[10.07086953  6.65138633  5.06005849  0.12167152  0.02895172]\n",
    "\n",
    " The Indices of the maximum Coefficient values: \n",
    "\n",
    "[2 4 1 3 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### For Ridge Regression using GridSearch obtained from above::\n",
    " The maximum Coefficient values: \n",
    "\n",
    "[6.69635381e+00 1.06661918e-01 8.84709707e-02 1.89214737e-02\n",
    " 5.48353351e-03]\n",
    "\n",
    " The Indices of the maximum Coefficient values: \n",
    "\n",
    "[2 0 1 4 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### For Lasso Regression using Grid Search obtained from above:\n",
    " The maximum Coefficient values: \n",
    "\n",
    "[6.70514057e+00 1.05630210e-01 8.73115691e-02 1.75780490e-02\n",
    " 4.43979327e-03]\n",
    "\n",
    " The Indices of the maximum Coefficient values: \n",
    "\n",
    "[2 0 1 4 3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For ElasticNet Regression using Grid Search obtained from above:\n",
    "The maximum Coefficient values: \n",
    "\n",
    "[6.70173765e+00 1.06150351e-01 8.78922987e-02 1.82395330e-02\n",
    " 4.98138045e-03]\n",
    "\n",
    " The Indices of the maximum Coefficient values: \n",
    "\n",
    "[2 0 1 4 3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the indices obtained for top 5 coefficients from Forward Search is the same but the coefficient values are different from the ones obtained from Grid Search Algorithm for all Ridge, Lasso and Elastic Net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, May 24 2022, 21:13:51) \n[Clang 13.1.6 (clang-1316.0.21.2)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
