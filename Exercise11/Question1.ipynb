{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61792faf",
   "metadata": {},
   "source": [
    "# 1) Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00813c39-3e86-4761-9aee-9c84aacad130",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3ed9922-2e2d-4036-8edf-ab4b7d7f3103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                                                #Importing Regular Expression\n",
    "import string                                            #Importing String\n",
    "import numpy as np                                       #Importing Numpy\n",
    "import pandas as pd                                      #Importing Pandas\n",
    "import random                                            #Importing Random\n",
    "from sklearn.datasets import fetch_20newsgroups          #Importing News Dataset\n",
    "from sklearn.svm import SVC                              #Importing SVM\n",
    "from sklearn.model_selection import GridSearchCV         #Importing Grid Search\n",
    "from sklearn.metrics import accuracy_score               #Importing Accuracy Score Metric\n",
    "from nltk.corpus import stopwords                        #Importing Stopwords\n",
    "from nltk.tokenize import word_tokenize                  #Importing NLTK Word Tokenizer\n",
    "import itertools                                         #Importing Iterator\n",
    "import warnings                                          #Importing Warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abbd22e7-0fff-4985-b78a-ea8a69e02589",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1bc0a0-7bb1-4d08-81b5-58520d60668a",
   "metadata": {},
   "source": [
    "#### Reading the Dataset and taking Subset with two categories named as sci.med and comp.graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b90ea04e-a52b-4b57-a240-81f3dfa2fb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the training data\n",
    "imdb_data=pd.read_csv('IMDB Dataset.csv')\n",
    "print(imdb_data.shape)\n",
    "imdb_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c8c0331",
   "metadata": {},
   "source": [
    "##### Taking a subset of the data as it takes a lot of computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a1b3257",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data = imdb_data[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ebc629-edab-41e5-b850-541dcb26a974",
   "metadata": {},
   "source": [
    "#### Preprocessing textual data to remove punctuation, stop-words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d583ff7d-6f63-45ad-be99-c30872656051",
   "metadata": {},
   "source": [
    "##### Function to Preprocess the data by removing Stopwords, punctuations and tokenizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c176a73-be1b-4007-9ad9-f8165d45bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    #Extracting the English stopwords and converting it into a set\n",
    "    english_stop_words = set(stopwords.words('english'))\n",
    "    #Making the data into the lower case string and then tokenizing the data into word list\n",
    "    data = word_tokenize(data.lower())\n",
    "    #Removing stopwords and punctuations from the word list\n",
    "    data = [word for word in data if word not in english_stop_words and word.isalpha()]\n",
    "    #Returning the final processed data list\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abed7dfa-0d90-4439-9350-4825c1dfd0ee",
   "metadata": {},
   "source": [
    "##### Extracting Movie Review and its Target label \"Sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ba20fe3-51b8-430c-b6c3-97d9529e9ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_review = imdb_data['review']\n",
    "sentiment = imdb_data['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8534b2-eb0c-412e-82be-9588b7bcfc34",
   "metadata": {},
   "source": [
    "##### Applying Preprocessing step on all the News items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "744c78bb-223a-430a-a20e-aff763722e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing an empty list to store processed movie reviews\n",
    "processed_movie_reviews = []\n",
    "#Iterating for each movie review\n",
    "for review in movie_review:\n",
    "    #Applying preprocessing on current movie review\n",
    "    processed_movie_reviews.append(preprocess_data(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea64f9-af41-4d33-8e30-aef6abde1a2e",
   "metadata": {},
   "source": [
    "#### Implementing a bag-of-words feature representation for each text sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2c94c-cb55-4215-b6d2-bca0e00c6a04",
   "metadata": {},
   "source": [
    "##### Function to create a Word Frequency Dictionary from the Provided Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84b211a5-db75-457e-837a-f4651a25685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq_dictionary(data, freq_dict):\n",
    "    #Iterating for all words in the document\n",
    "    for word in data:\n",
    "        #If word is already present in the dictionary then add 1\n",
    "        if word in freq_dict:\n",
    "            freq_dict[word] += 1\n",
    "        #If word is not present, then create a new key and assign 1 as it's value\n",
    "        else:\n",
    "            freq_dict[word] = 1\n",
    "    #Returning the created word frequency dictionary\n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332f5b74-556e-4030-846b-2eceb91bbe56",
   "metadata": {},
   "source": [
    "##### Function to create a Binary vector for a document based on Bag of Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5590971-ba18-4587-bf3f-b9d37c4c37b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(data, freq_dict):\n",
    "    #Initializing a vector with zeros having the length equal to total unique words in the corpus\n",
    "    vector = np.zeros(shape=(len(freq_dict.keys()),))\n",
    "    #Iterating over all words in the document\n",
    "    for word in data:\n",
    "        #Placing 1 in the vector based on index assigned to that word in the dictionary\n",
    "        if word in freq_dict.keys():\n",
    "            vector[list(freq_dict.keys()).index(word)] = 1\n",
    "    #Returning the document vector\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccbff4b-e6f2-4326-9a15-1c48a6e29be3",
   "metadata": {},
   "source": [
    "##### Converting each document in the dataset into a Bag of Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd08ef1c-fd60-4c75-bb99-44c9c9993c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total Features to consider for Processing\n",
    "features = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b06d8660-2dd6-41d6-8b31-231813e3ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dictionary to store all unique words and there respective counts in the entire corpus\n",
    "corpus_word_freq_dictionary = {}\n",
    "#Iterating for each document in the dataset\n",
    "for doc in processed_movie_reviews:\n",
    "    #Updating the dictionary for each document\n",
    "    corpus_word_freq_dictionary = word_freq_dictionary(doc, corpus_word_freq_dictionary)\n",
    "#Sorting the dictionary in Descending Order\n",
    "corpus_word_freq_dictionary = dict(sorted(corpus_word_freq_dictionary.items(), key=lambda item: item[1], reverse=True))\n",
    "#Extracting only required words based on feature value\n",
    "corpus_word_freq_dictionary = dict(itertools.islice(corpus_word_freq_dictionary.items(), features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0593fefe-769c-4f04-9e76-09f8962e9af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing a empty list to store the Bag of Word representation for each document\n",
    "movie_review_bog = []\n",
    "#Iterating for each document in the dataset\n",
    "for document in processed_movie_reviews:\n",
    "    #Creating a bag of word representation vector and appending it into the final list\n",
    "    movie_review_bog.append(bag_of_words(document, corpus_word_freq_dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231c72c1-d902-4ad7-b525-46bc854c958d",
   "metadata": {},
   "source": [
    "##### Displaying the created Bag of Word Representation in the form of Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62cf8c35-b296-4eaa-994b-0727dd7ad6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>br</th>\n",
       "      <th>movie</th>\n",
       "      <th>film</th>\n",
       "      <th>one</th>\n",
       "      <th>like</th>\n",
       "      <th>good</th>\n",
       "      <th>would</th>\n",
       "      <th>even</th>\n",
       "      <th>see</th>\n",
       "      <th>really</th>\n",
       "      <th>...</th>\n",
       "      <th>incredibly</th>\n",
       "      <th>convincing</th>\n",
       "      <th>terms</th>\n",
       "      <th>recommended</th>\n",
       "      <th>event</th>\n",
       "      <th>genius</th>\n",
       "      <th>sadly</th>\n",
       "      <th>red</th>\n",
       "      <th>writers</th>\n",
       "      <th>cover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    br  movie  film  one  like  good  would  even  see  really  ...  \\\n",
       "0  1.0    0.0   0.0  1.0   0.0   0.0    1.0   0.0  0.0     0.0  ...   \n",
       "1  1.0    0.0   0.0  1.0   0.0   0.0    0.0   0.0  1.0     1.0  ...   \n",
       "2  1.0    0.0   0.0  1.0   0.0   0.0    0.0   1.0  1.0     0.0  ...   \n",
       "3  1.0    1.0   1.0  0.0   1.0   0.0    0.0   0.0  1.0     0.0  ...   \n",
       "4  1.0    1.0   1.0  1.0   0.0   1.0    0.0   0.0  1.0     0.0  ...   \n",
       "\n",
       "   incredibly  convincing  terms  recommended  event  genius  sadly  red  \\\n",
       "0         0.0         0.0    0.0          0.0    0.0     0.0    0.0  0.0   \n",
       "1         0.0         0.0    0.0          0.0    0.0     0.0    0.0  0.0   \n",
       "2         0.0         0.0    0.0          0.0    0.0     0.0    0.0  0.0   \n",
       "3         0.0         0.0    0.0          0.0    0.0     0.0    0.0  0.0   \n",
       "4         0.0         0.0    0.0          0.0    0.0     0.0    0.0  0.0   \n",
       "\n",
       "   writers  cover  \n",
       "0      0.0    0.0  \n",
       "1      0.0    0.0  \n",
       "2      0.0    0.0  \n",
       "3      0.0    0.0  \n",
       "4      0.0    0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_review_bog_dataframe = pd.DataFrame(movie_review_bog, columns=corpus_word_freq_dictionary.keys())\n",
    "movie_review_bog_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f66d1a-17e3-4cfe-ae2e-143bde2598ab",
   "metadata": {},
   "source": [
    "#### Implementing a TF-IDF feature representation for each text sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7078406-90e1-4ff0-8dfb-13c00b6ad755",
   "metadata": {},
   "source": [
    "##### Function to calculate the Term Frequency (TF) for a word in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e0a02941-a542-48fb-a5a9-3f742e85ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(document, word):\n",
    "    #tf = (total frequency of word in a document) / (total words in a document)\n",
    "    return document[word]/sum(document.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5e4ff6-6f6e-43b0-b647-977020e125b9",
   "metadata": {},
   "source": [
    "##### Function to calculate the Inverse Document Frequency (IDF) for a word in the entire Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "878f46d1-f46e-40d7-9be5-fc100e09d6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_document_frequency(total_doc_freq_dictionary, word, total_documents):\n",
    "    #idf = log(total documents / total frequency of word in all documents)\n",
    "    return np.log(total_documents/total_doc_freq_dictionary[word] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a5880e-eb7a-46ff-b28c-519c90e4de06",
   "metadata": {},
   "source": [
    "##### Function to calculate the TF-IDF of a all the words individually in the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01197f67-0520-4c34-91d0-134189f0548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(document, total_doc_freq_dictionary, total_documents):\n",
    "    #Initializing a vector of zeros with a lenght of total unique words in the entire corpus\n",
    "    vector = np.zeros(shape=(len(total_doc_freq_dictionary.keys()),))\n",
    "    #Iterating for each word in the document\n",
    "    for word in document.keys():\n",
    "        #Checking if word exist in our feature set\n",
    "        if word in total_doc_freq_dictionary.keys():\n",
    "            # tf-idf = tf * idf\n",
    "            tf_idf = term_frequency(document, word) * inverse_document_frequency(total_doc_freq_dictionary, word, total_documents)\n",
    "            #Inserting the calculated tf-idf for that word in the vector\n",
    "            vector[list(total_doc_freq_dictionary.keys()).index(word)] = tf_idf \n",
    "    #Returning the final vector containing tf-idf values\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72869cb-9468-4925-9223-814961133a5e",
   "metadata": {},
   "source": [
    "##### Converting each document in the dataset into a TF-IDF Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25185765-4714-4902-aa19-8720d937e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing a vector to store tf-idf vectors for each document\n",
    "movie_review_tfidf_vectors = []\n",
    "#Iterating for each documents\n",
    "for document in processed_movie_reviews:\n",
    "    #Creating a word frequency dictionary for current document\n",
    "    current_document_dict = word_freq_dictionary(document, {})\n",
    "    #Calculating and Appending the tf-idf vector in the final vector\n",
    "    movie_review_tfidf_vectors.append(tf_idf(current_document_dict, corpus_word_freq_dictionary, len(processed_movie_reviews)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185d3492-d1d1-4dec-ac13-dc6bafbd16bd",
   "metadata": {},
   "source": [
    "##### Displaying the created TF-IDF Representation in the form of Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e6c09c1-830e-4d7e-885b-c6f67381ca5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>br</th>\n",
       "      <th>movie</th>\n",
       "      <th>film</th>\n",
       "      <th>one</th>\n",
       "      <th>like</th>\n",
       "      <th>good</th>\n",
       "      <th>would</th>\n",
       "      <th>even</th>\n",
       "      <th>see</th>\n",
       "      <th>really</th>\n",
       "      <th>...</th>\n",
       "      <th>incredibly</th>\n",
       "      <th>convincing</th>\n",
       "      <th>terms</th>\n",
       "      <th>recommended</th>\n",
       "      <th>event</th>\n",
       "      <th>genius</th>\n",
       "      <th>sadly</th>\n",
       "      <th>red</th>\n",
       "      <th>writers</th>\n",
       "      <th>cover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.015399</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013259</td>\n",
       "      <td>0.01365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012976</td>\n",
       "      <td>0.013575</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.019766</td>\n",
       "      <td>0.020536</td>\n",
       "      <td>0.015418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017019</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013904</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>0.008134</td>\n",
       "      <td>0.033118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016335</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008979</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         br     movie      film       one      like      good     would  \\\n",
       "0  0.007883  0.000000  0.000000  0.004173  0.000000  0.000000  0.012555   \n",
       "1  0.015399  0.000000  0.000000  0.008151  0.000000  0.000000  0.000000   \n",
       "2  0.010510  0.000000  0.000000  0.008345  0.000000  0.000000  0.000000   \n",
       "3  0.019766  0.020536  0.015418  0.000000  0.012624  0.000000  0.000000   \n",
       "4  0.013904  0.003611  0.008134  0.033118  0.000000  0.016335  0.000000   \n",
       "\n",
       "       even       see   really  ...  incredibly  convincing  terms  \\\n",
       "0  0.000000  0.000000  0.00000  ...         0.0         0.0    0.0   \n",
       "1  0.000000  0.013259  0.01365  ...         0.0         0.0    0.0   \n",
       "2  0.012976  0.013575  0.00000  ...         0.0         0.0    0.0   \n",
       "3  0.000000  0.017019  0.00000  ...         0.0         0.0    0.0   \n",
       "4  0.000000  0.008979  0.00000  ...         0.0         0.0    0.0   \n",
       "\n",
       "   recommended  event  genius  sadly  red  writers  cover  \n",
       "0          0.0    0.0     0.0    0.0  0.0      0.0    0.0  \n",
       "1          0.0    0.0     0.0    0.0  0.0      0.0    0.0  \n",
       "2          0.0    0.0     0.0    0.0  0.0      0.0    0.0  \n",
       "3          0.0    0.0     0.0    0.0  0.0      0.0    0.0  \n",
       "4          0.0    0.0     0.0    0.0  0.0      0.0    0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moview_reviews_tfidf_dataframe = pd.DataFrame(movie_review_tfidf_vectors, columns=corpus_word_freq_dictionary.keys())\n",
    "moview_reviews_tfidf_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbe4dde-19b6-46de-ba6e-fcd43674a59a",
   "metadata": {},
   "source": [
    "#### Splitting the dataset randomly into train/validation/test splits according to ratios 80%:10%:10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48a4cc7f-65cd-4ac3-b8a8-87ebb2b319a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(moview_review, sentiment, train_ratio, validation_ratio):\n",
    "    #Combining the Feature columns and the target column into a single list\n",
    "    combined = list(zip(moview_review, sentiment))\n",
    "    #Randomly shuffle the rows in the list\n",
    "    random.shuffle(combined)\n",
    "    #Calculating the training rows and validation rows\n",
    "    train_rows = int(len(combined) * train_ratio)\n",
    "    validation_rows = int(len(combined) * validation_ratio)\n",
    "    #Extracting X matrix and Y matrix from the combined list\n",
    "    X , y = list(zip(*combined))\n",
    "    X, y = list(X), list(y)\n",
    "    #Splitting X and y matrices into Training, Validation and Test set\n",
    "    X_train, X_val, X_test = X[:train_rows], X[train_rows:train_rows+validation_rows], X[train_rows+validation_rows:]\n",
    "    y_train, y_val, y_test = y[:train_rows], y[train_rows:train_rows+validation_rows], y[train_rows+validation_rows:]\n",
    "    #Returning all the subsets\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630baf3b-e39e-4009-89cd-354ebdde74a7",
   "metadata": {},
   "source": [
    "### Exercise 1: Implementing Naive Bayes Classifier for Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bdb051-cdaa-4467-ae70-b9e559804ad8",
   "metadata": {},
   "source": [
    "#### Class to Represent the Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8dedba9e-39ee-47b7-8239-3cc9d4f0ef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Naive_bayes:\n",
    "    #Contructor function\n",
    "    def __init__(self,feature_representation, dataset, target):\n",
    "        #Checking if the feature representation value is valid otherwise Raising Exception\n",
    "        if feature_representation not in ['bog', 'tfidf']:\n",
    "            raise Exception('Invalid value provided for Feature Representation')\n",
    "        #Feature Representation type - Bag of Words (bog) or TF-IDF (tfidf)\n",
    "        self.feature_representation = feature_representation\n",
    "        #Feature Columns of the Dataset\n",
    "        self.dataset = np.array(dataset)\n",
    "        #Target/Label Column of the Dataset\n",
    "        self.target = np.array(target)\n",
    "        #Unique Target/Label values\n",
    "        self.unique_target = list(set(target))\n",
    "        #Current Accuarcy of the Model\n",
    "        self.accuracy = 0\n",
    "        #Combined Dataset containing both, the Feature Columns and the Target Column\n",
    "        self.combined = np.concatenate((self.dataset, self.target.reshape(-1,1)), axis = 1)\n",
    "    \n",
    "    #Function to find Predictions on the dataset provided and calculate the Accuracy\n",
    "    def predict(self):\n",
    "        #Iterating over all the different documents in the dataset\n",
    "        for index, document in enumerate(self.combined):\n",
    "            #Initializing an empty list to store the predicted probability for each target value\n",
    "            target_probability = []\n",
    "            #Iterating over all unique target values for calculating there probabilities\n",
    "            for target in self.unique_target:\n",
    "                #Based on Feature representation type, calculating the probabiltiy\n",
    "                if self.feature_representation == 'bog':\n",
    "                    target_probability.append(self.calculate_probability_bog(document, target))\n",
    "                elif self.feature_representation == 'tfidf':\n",
    "                    target_probability.append(self.calculate_probability_tfidf(document, target))\n",
    "            #Normalizing each probability so that it sums to 1\n",
    "            target_probability = list(map(lambda x : x / (sum(target_probability) + 1),target_probability))\n",
    "            #Extracting the class with the maximum Probability\n",
    "            predicted_class = self.unique_target[target_probability.index(max(target_probability))]\n",
    "            #Checking if the predicted class is equal to the actual class\n",
    "            if predicted_class == self.target[index]:\n",
    "                self.accuracy += 1\n",
    "        #Calculating its final accuracy\n",
    "        self.accuracy /= len(self.combined)\n",
    "\n",
    "\n",
    "    #A private function to calculate the Probability for a document represented using Bag of Words\n",
    "    def calculate_probability_bog(self, document, target):\n",
    "        #Calculating the probability for a class itself\n",
    "        prob_class = len(self.target[np.where(self.target == target)]) / len(self.target)\n",
    "        #Initializing the prior probability for each Word in the document\n",
    "        prior_word_probability = 1\n",
    "        #Iterating over each word in the document\n",
    "        for i in range(len(document) - 1):\n",
    "            #Calculating the probability only if the word exist in the document\n",
    "            if document[i] == 1:\n",
    "                #Calculating the Prior probability of the word given the class\n",
    "                # P(w1 | target) = (Number of times the word occurs in all the document given the class) / (Number of time word occurs in all documents)\n",
    "                numerator = len(self.combined[np.where((self.combined[:,i] == 1) & (self.combined[:,-1] == target))])\n",
    "                denominator = len(self.combined[np.where(self.combined[:,-1] == target)])\n",
    "                #Multiplying the current word prior probability with other words\n",
    "                prior_word_probability *= (numerator / denominator)\n",
    "        #Returning the final probability -> P(class) * P(W | class)\n",
    "        return prior_word_probability * prob_class\n",
    "    \n",
    "    #A private function to calculate the Probability for a document represented using TF-IDF\n",
    "    def calculate_probability_tfidf(self, document, target):\n",
    "        #Calculating the probability for a class itself\n",
    "        # P(target) = (Number of times that class appears in the dataset) / (total documents)\n",
    "        prob_class = len(self.target[np.where(self.target == target)]) / len(self.target)\n",
    "        #Initializing the prior probability for each Word in the document\n",
    "        prior_word_probability = 1\n",
    "        #Iterating over each word in the document\n",
    "        for i in range(len(document) - 1):\n",
    "            #Only calculating the probability if the word exist in the document\n",
    "            if document[i] == 1:\n",
    "                #Calculating the Prior probability of the word given the class\n",
    "                # P(w1 | target) = (Number of times the word occurs in all the document given the class) / (Number of time word occurs in all documents)\n",
    "                numerator = sum(self.combined[np.where((self.combined[:,i] == 1) & (self.combined[:,-1] == target))])\n",
    "                denominator = sum(self.combined[np.where(self.combined[:,-1] == target)])\n",
    "                #Multiplying the current word prior probability with other words\n",
    "                prior_word_probability *= (numerator / denominator)\n",
    "        #Returning the final probability -> P(class) * P(W | class)\n",
    "        return prior_word_probability * prob_class\n",
    "    \n",
    "    #Function to display the Accuracy of the Model\n",
    "    def score(self):\n",
    "        feature_representation = 'Bag of Words' if self.feature_representation == 'bog' else 'TF-IDF'\n",
    "        return 'The Accuracy for Naive Bayes using {} Representation is {:.2f}%'.format(feature_representation, self.accuracy * 100)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7cc430-67dc-402c-bdc3-ffc61dc27d88",
   "metadata": {},
   "source": [
    "#### Using Bag of Word Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e5ddb7-6e52-40fd-9d4e-4815f0e01171",
   "metadata": {},
   "source": [
    "##### Splitting the Dataset with Bag of Word Representation into Train, Validation and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "421efc99-39b5-4b68-a9af-5e2d094e93b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(movie_review_bog, sentiment, 0.8, 0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba257f97-5945-4477-8bb3-f7d7297ef311",
   "metadata": {},
   "source": [
    "##### Creating and Fitting the Naive Bayes model on the training, Validation and Test Sets\n",
    "##### Calculating and Displaying the Training, Validation and Test Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ad32d8d1-c45c-49f8-a509-5757e541d1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: \n",
      "The Accuracy for Naive Bayes using Bag of Words Representation is 50.25%\n",
      "\n",
      "Validation Accuracy: \n",
      "The Accuracy for Naive Bayes using Bag of Words Representation is 51.00%\n",
      "\n",
      "Test Accuracy: \n",
      "The Accuracy for Naive Bayes using Bag of Words Representation is 50.50%\n"
     ]
    }
   ],
   "source": [
    "nb_model_train = Naive_bayes('bog', X_train, y_train)\n",
    "nb_model_train.predict()\n",
    "print('Training Accuracy: \\n{}'.format(nb_model_train.score()))\n",
    "\n",
    "nb_model_validation = Naive_bayes('bog', X_val, y_val)\n",
    "nb_model_validation.predict()\n",
    "print('\\nValidation Accuracy: \\n{}'.format(nb_model_validation.score()))\n",
    "\n",
    "nb_model_test = Naive_bayes('bog', X_test, y_test)\n",
    "nb_model_test.predict()\n",
    "print('\\nTest Accuracy: \\n{}'.format(nb_model_test.score()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc235a-2b19-40bf-bc51-9f0aa079d8af",
   "metadata": {},
   "source": [
    "#### Using TF-IDF Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b39a7-2f23-4337-a8b9-f314aba30fce",
   "metadata": {},
   "source": [
    "##### Splitting the Dataset with TF-IDF Representation into Train, Validation and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "628ac9e0-fb52-4227-b2b3-1230f990da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(movie_review_tfidf_vectors, sentiment, 0.8, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c791a56-3ffd-47cc-856d-b3ce598e23ff",
   "metadata": {},
   "source": [
    "##### Creating and Fitting the Naive Bayes model on the training, Validation and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4d8fc0-b20c-4653-9e2e-51ff0d77fded",
   "metadata": {},
   "source": [
    "##### Calculating and Displaying the Training, Validation and Test Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d5586e0-9ea4-4c30-b7bd-52a933115244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: \n",
      "The Accuracy for Naive Bayes using TF-IDF Representation is 50.19%\n",
      "\n",
      "Validation Accuracy: \n",
      "The Accuracy for Naive Bayes using TF-IDF Representation is 52.00%\n",
      "\n",
      "Test Accuracy: \n",
      "The Accuracy for Naive Bayes using TF-IDF Representation is 56.00%\n"
     ]
    }
   ],
   "source": [
    "nb_model_train = Naive_bayes('tfidf', X_train, y_train)\n",
    "nb_model_train.predict()\n",
    "print('Training Accuracy: \\n{}'.format(nb_model_train.score()))\n",
    "\n",
    "nb_model_validation = Naive_bayes('tfidf', X_val, y_val)\n",
    "nb_model_validation.predict()\n",
    "print('\\nValidation Accuracy: \\n{}'.format(nb_model_validation.score()))\n",
    "\n",
    "nb_model_test = Naive_bayes('tfidf', X_test, y_test)\n",
    "nb_model_test.predict()\n",
    "print('\\nTest Accuracy: \\n{}'.format(nb_model_test.score()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8353d0d-76e6-4ecf-b786-fa003a25bd7c",
   "metadata": {},
   "source": [
    "#### The Accuracy of TF-IDF is low because we have only taken 1000 features.If we increase the number of features, the accuracy can improve subsequently."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "387f0543-cf58-4d8a-9821-f0f544507841",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Implementing SVM Classifier via Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ff4a33-66df-4ae6-81b5-c4978981ebb9",
   "metadata": {},
   "source": [
    "#### Defining Hyperparameter Grid for Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a352914-e0fb-49c8-bae1-2da6767c60f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_grid = {'C' : [0.01,0.02,0.03],'kernel': ['linear', 'rbf'],'gamma': ['scale','auto']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f6953-a0ec-42e2-b485-7782cc446025",
   "metadata": {},
   "source": [
    "#### Using Bag of Word Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a3ff36-7bec-455a-b387-5ef2db1533a1",
   "metadata": {},
   "source": [
    "##### Splitting the Dataset with Bag of Word Representation into Train, Validation and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c74658f-2271-465f-8749-3902e1ade769",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(movie_review_bog, sentiment, 0.8, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bb1b6f-9436-4009-ab7b-316dd2630e43",
   "metadata": {},
   "source": [
    "##### Creating and Fitting the SVM model on the training set using Grid Search and different Hyperparameter combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fa4c97f5-8c0f-4c02-b632-e06ab565ea87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=SVC(random_state=2023), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [0.01, 0.02, 0.03], &#x27;gamma&#x27;: [&#x27;scale&#x27;, &#x27;auto&#x27;],\n",
       "                         &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;]},\n",
       "             return_train_score=True, scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=SVC(random_state=2023), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [0.01, 0.02, 0.03], &#x27;gamma&#x27;: [&#x27;scale&#x27;, &#x27;auto&#x27;],\n",
       "                         &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;]},\n",
       "             return_train_score=True, scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(random_state=2023)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(random_state=2023)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(random_state=2023), n_jobs=-1,\n",
       "             param_grid={'C': [0.01, 0.02, 0.03], 'gamma': ['scale', 'auto'],\n",
       "                         'kernel': ['linear', 'rbf']},\n",
       "             return_train_score=True, scoring='accuracy')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initializing a SVM model with the random seed\n",
    "random_seed = 2023\n",
    "svm = SVC(random_state=random_seed)\n",
    "#Creating a Grid Seach object with the SVM model and K-fold Cross validation\n",
    "model = GridSearchCV(svm, hyperparameter_grid, n_jobs=-1, cv=5, scoring='accuracy', return_train_score=True)\n",
    "#Fitting the training dataset on SVM with different Hyperparameters\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "726fb72d-2d86-4974-9d4d-5133a3d89414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameter combination found for Bag of Word Representation after applying Grid Search: \n",
      "{'C': 0.02, 'gamma': 'scale', 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "print('Best Hyperparameter combination found for Bag of Word Representation after applying Grid Search: \\n{}'.format(model.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59caeeb8-6a03-4a19-9825-7dab97de1fbb",
   "metadata": {},
   "source": [
    "##### Calculating and Displaying the Validation and Test Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8fb4990-2e62-47da-92d1-d6c43ce2371b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy on best Hyperparameters: 82.00\n",
      "Test Accuracy on best Hyperparameters: 84.00\n"
     ]
    }
   ],
   "source": [
    "print('Validation Accuracy on best Hyperparameters: {:.2f}'.format(accuracy_score(y_val, model.predict(X_val)) * 100))\n",
    "print('Test Accuracy on best Hyperparameters: {:.2f}'.format(accuracy_score(y_test, model.predict(X_test)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fafea4-9c8f-4750-bdad-dc835f430f39",
   "metadata": {},
   "source": [
    "#### Using TF-IDF Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6ad22f-87cd-44fc-b2fc-7bf40de2da68",
   "metadata": {},
   "source": [
    "##### Splitting the Dataset with TF-IDF Representation into Train, Validation and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f4cb7e43-bed1-474a-80ca-7b2ac47a5519",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(movie_review_tfidf_vectors, sentiment, 0.8, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b42a9e-84ee-4fa8-8877-af00d72ca664",
   "metadata": {},
   "source": [
    "##### Creating and Fitting the SVM model on the training set using Grid Search and different Hyperparameter combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d47f92cf-2bfa-400d-933d-843f9ed18b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=SVC(random_state=2023), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [0.01, 0.02, 0.03], &#x27;gamma&#x27;: [&#x27;scale&#x27;, &#x27;auto&#x27;],\n",
       "                         &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;]},\n",
       "             return_train_score=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=SVC(random_state=2023), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [0.01, 0.02, 0.03], &#x27;gamma&#x27;: [&#x27;scale&#x27;, &#x27;auto&#x27;],\n",
       "                         &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;]},\n",
       "             return_train_score=True)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(random_state=2023)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(random_state=2023)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(random_state=2023), n_jobs=-1,\n",
       "             param_grid={'C': [0.01, 0.02, 0.03], 'gamma': ['scale', 'auto'],\n",
       "                         'kernel': ['linear', 'rbf']},\n",
       "             return_train_score=True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initializing a SVM model with the random seed\n",
    "svm = SVC(random_state=random_seed)\n",
    "#Creating a Grid Seach object with the SVM model and K-fold Cross validation\n",
    "model1 = GridSearchCV(svm, hyperparameter_grid, n_jobs=-1, cv=5, return_train_score=True)\n",
    "#Fitting the training dataset on SVM with different Hyperparameters\n",
    "model1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aeabdce4-44a4-481f-a98d-f020fa103ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameter combination found for TF-IDF Representation after applying Grid Search: \n",
      "{'C': 0.01, 'gamma': 'scale', 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "print('Best Hyperparameter combination found for TF-IDF Representation after applying Grid Search: \\n{}'.format(model1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc42131d-fb02-451a-9a6b-deac8372faa4",
   "metadata": {},
   "source": [
    "##### Calculating and Displaying the Validation and Test Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7dbefa63-0dca-4e94-823d-6c739c0bc58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy on best Hyperparameters: 47.50\n",
      "Test Accuracy on best Hyperparameters: 49.50\n"
     ]
    }
   ],
   "source": [
    "print('Validation Accuracy on best Hyperparameters: {:.2f}'.format(accuracy_score(y_val, model1.predict(X_val)) * 100))\n",
    "print('Test Accuracy on best Hyperparameters: {:.2f}'.format(accuracy_score(y_test, model1.predict(X_test)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbdbc09-eb4e-4a58-b73c-74e1ee3f212a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, May 24 2022, 21:13:51) \n[Clang 13.1.6 (clang-1316.0.21.2)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
